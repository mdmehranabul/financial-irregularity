{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6515dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0e85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory information\n",
    "DATA_DIR = \"data\"  # Directory containing input CSV files\n",
    "OUTPUT_DIR = \"output\" # Directory to store output CSV files\n",
    "RESOLVED_DIR = os.path.join(OUTPUT_DIR, \"resolved\")\n",
    "UNRESOLVED_DIR = os.path.join(OUTPUT_DIR, \"unresolved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20657607",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESOLVED_DIR, exist_ok=True)\n",
    "os.makedirs(UNRESOLVED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a3ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment\t   CerebroSA-switchback  Icebreaker    Test\r\n",
      "Assignment.ipynb   ChatBot.ipynb\t LLM Practice  Untitled1.ipynb\r\n",
      "CerebroArabic_LLM  data\t\t\t lost+found    Untitled.ipynb\r\n",
      "CerebroEN\t   GL_groceries\t\t output\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad3829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_categorize(input_file):\n",
    "    df = pd.read_csv(input_file).head(200)\n",
    "\n",
    "    # Handle missing/invalid data\n",
    "    df['sys_a_amount_attribute_1'].fillna(0, inplace=True)\n",
    "    df['sys_b_amount_attribute_1'].fillna(0, inplace=True)\n",
    "    df['sys_a_date'] = pd.to_datetime(df['sys_a_date'], errors='coerce')\n",
    "    df['sys_b_date'] = pd.to_datetime(df['sys_b_date'], errors='coerce')\n",
    "    \n",
    "    # Convert recon_sub_status to dictionary and extract categories\n",
    "    def extract_status(status, category):\n",
    "        try:\n",
    "            status_dict = json.loads(status.replace(\"'\", '\"'))  # Convert single to double quotes if needed\n",
    "            return category in status_dict.values()\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return False  # If parsing fails, assume it's not a match\n",
    "\n",
    "    df['is_matched'] = df['recon_sub_status'].apply(lambda x: extract_status(x, \"Matched\"))\n",
    "    df['is_not_found_sys_b'] = df['recon_sub_status'].apply(lambda x: extract_status(x, \"Not Found-SysB\"))\n",
    "    df['is_not_matched'] = df['recon_sub_status'].apply(lambda x: extract_status(x, \"Not Matched\"))\n",
    "\n",
    "    # Filter the data into separate categories\n",
    "    matched = df[df['is_matched']][['txn_ref_id', 'sys_a_amount_attribute_1', 'sys_a_date']].copy()\n",
    "    not_found_sys_b = df[df['is_not_found_sys_b']][['txn_ref_id', 'sys_a_amount_attribute_1', 'sys_a_date']].copy()\n",
    "    not_matched = df[df['is_not_matched']][['txn_ref_id', 'sys_a_amount_attribute_1', 'sys_a_date']].copy()\n",
    "    \n",
    "    # Format date column\n",
    "    for category_df in [matched, not_found_sys_b, not_matched]:\n",
    "        category_df['date'] = category_df['sys_a_date'].dt.strftime('%Y-%m-%d')\n",
    "        category_df.drop(columns=['sys_a_date'], inplace=True)\n",
    "        category_df.rename(columns={'txn_ref_id': 'order_id'}, inplace=True) #Rename Here\n",
    "    \n",
    "    return matched, not_found_sys_b, not_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c5eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"File uploaded to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f1d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = pipeline('text2text-generation', model='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resolution(resolution_file, not_found_data):\n",
    "    resolution_df = pd.read_csv(resolution_file, encoding=\"ISO-8859-1\").head(200)\n",
    "    merged_df = pd.merge(not_found_data, resolution_df, left_on='order_id', right_on='Transaction ID', how='left')\n",
    "    \n",
    "    # Lists to store resolved and unresolved rows\n",
    "    resolved_cases = []\n",
    "    unresolved_cases = []\n",
    "    unresolved_details = []  # New list to store summary & next steps\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        order_id = row['order_id']\n",
    "        amount = row['amount']\n",
    "        date = row['date']\n",
    "        comment = str(row.get('Comments', '')).strip()  # Ensure comment is a string\n",
    "\n",
    "        if re.search(r'\\bresolved\\b', comment, re.IGNORECASE):  \n",
    "            resolved_cases.append([order_id, amount, date, comment])\n",
    "        else:\n",
    "            summary = generate_summary(comment, llm)  \n",
    "            next_steps = suggest_next_steps(comment, llm)  \n",
    "            unresolved_cases.append([order_id, amount, date, comment])\n",
    "            unresolved_details.append([order_id, comment, summary, next_steps])  # Store in new file\n",
    "\n",
    "    # Convert lists to DataFrames\n",
    "    resolved_df = pd.DataFrame(resolved_cases, columns=['order_id', 'amount', 'date', 'comment'])\n",
    "    unresolved_df = pd.DataFrame(unresolved_cases, columns=['order_id', 'amount', 'date', 'comment'])\n",
    "    unresolved_details_df = pd.DataFrame(unresolved_details, columns=['order_id', 'comment', 'summary', 'next_steps'])  # âœ… New DataFrame\n",
    "\n",
    "    # Save consolidated files\n",
    "    resolved_filepath = os.path.join(RESOLVED_DIR, \"resolved_cases.csv\")\n",
    "    unresolved_filepath = os.path.join(UNRESOLVED_DIR, \"unresolved_cases.csv\")\n",
    "    unresolved_details_filepath = os.path.join(UNRESOLVED_DIR, \"unresolved_cases_details.csv\")  # New file\n",
    "\n",
    "    resolved_df.to_csv(resolved_filepath, index=False)\n",
    "    unresolved_df.to_csv(unresolved_filepath, index=False)\n",
    "    unresolved_details_df.to_csv(unresolved_details_filepath, index=False)  #Save new file\n",
    "\n",
    "    print(f\"Consolidated Resolved cases saved at: {resolved_filepath}\")\n",
    "    print(f\"Consolidated Unresolved cases saved at: {unresolved_filepath}\")\n",
    "    print(f\"Unresolved Cases Details saved at: {unresolved_details_filepath}\")  # Print new file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0682cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0881f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_pattern(comment, llm):  # Add llm as an argument\n",
    "    prompt = f\"Identify the key pattern or reason for resolution in this financial discrepancy comment: {comment}\"\n",
    "    pattern = llm(prompt)[0]['generated_text']  # Extract from BART's output\n",
    "    return pattern.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88beabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(comment, llm):  # Add llm as an argument\n",
    "    prompt = f\"Summarize why this financial discrepancy is unresolved: {comment}\"\n",
    "    summary = llm(prompt)[0]['generated_text']  # Extract from BART's output\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06effe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_next_steps(comment, llm):  # Add llm as an argument\n",
    "    prompt = f\"Suggest next steps to resolve this financial discrepancy: {comment}\"\n",
    "    next_steps = llm(prompt)[0]['generated_text']  # Extract from BART's output\n",
    "    return next_steps.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2db55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(order_id, status, *args):  # *args for variable number of arguments\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_USER\n",
    "    msg['To'] = EMAIL_RECIPIENT\n",
    "    msg['Subject'] = f\"Financial Discrepancy Update - Order ID: {order_id}\"\n",
    "\n",
    "    body = f\"Order ID: {order_id}\\nStatus: {status}\\n\"\n",
    "    for arg in args:  # Add summary, next steps, or pattern to email body\n",
    "        body += f\"{arg}\\n\"\n",
    "\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)  # Example: Gmail\n",
    "        server.starttls()\n",
    "        server.login(EMAIL_USER, EMAIL_PASSWORD)\n",
    "        server.sendmail(EMAIL_USER, EMAIL_RECIPIENT, msg.as_string())\n",
    "        server.quit()\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Email sending failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d7cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to: ./output/matched.csv\n",
      "File uploaded to: ./output/not_found_sys_b.csv\n",
      "File uploaded to: ./output/not_matched.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Resolved cases saved at: output/resolved/resolved_cases.csv\n",
      "Consolidated Unresolved cases saved at: output/unresolved/unresolved_cases.csv\n",
      "Unresolved Cases Details saved at: output/unresolved/unresolved_cases_details.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = \"./data\"\n",
    "    OUTPUT_DIR = \"./output\"\n",
    "    \n",
    "    raw_data_file = os.path.join(DATA_DIR, \"recon_data_raw.csv\")\n",
    "    resolution_file = os.path.join(DATA_DIR, \"recon_data_reply.csv\")\n",
    "\n",
    "    matched_data, not_found_data, not_matched_data = preprocess_and_categorize(raw_data_file)\n",
    "    \n",
    "    matched_file_path = os.path.join(OUTPUT_DIR, \"matched.csv\")\n",
    "    not_found_file_path = os.path.join(OUTPUT_DIR, \"not_found_sys_b.csv\")\n",
    "    not_matched_file_path = os.path.join(OUTPUT_DIR, \"not_matched.csv\")\n",
    "    \n",
    "    upload_file(matched_data, matched_file_path)\n",
    "    upload_file(not_found_data, not_found_file_path)\n",
    "    upload_file(not_matched_data, not_matched_file_path)\n",
    "    \n",
    "    process_resolution(resolution_file, not_found_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57297ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
